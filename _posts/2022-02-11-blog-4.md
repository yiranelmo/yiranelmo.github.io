---
layout: post
title: Blog Post 4 - Spectral Clustering
---

In this blog post, we'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points.



# Introduction

In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering. 

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

ÊèíÂÖ•ÂõæÁâáÔºö

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

ÊèíÂÖ•ÂõæÁâá



# Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

ÊèíÂÖ•ÂõæÁâá

We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

ÊèíÂÖ•ÂõæÁâá

Whoops! That's not right! 

As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, you will derive and implement spectral clustering. 



# Part A

First, we need make a similarity matrix  `ùêÄ`.   `A ` should be a matrix (2d, `np.ndarray`) with shape `(n, n)`.

When constructing the similarity matrix, use a parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`), and `0` otherwise. 

**The diagonal entries `A[i,i]` should all be equal to 0.** 

**Note:**  `pairwise_distances ` function from `sklearn`.   And `np.fill_diagonal()` is a good way to set the values of the diagonal of a matrix.  

```python
# try use epsilon = 0.4
epsilon = 0.4

# pairwise distances matrix of X
from sklearn.metrics.pairwise import pairwise_distances
A = pairwise_distances(X, X)

# if within distance epsilon is 1, and 0 otherwise
A[A < epsilon] = 1
A[A != 1] = 0

# set diagonal entries to 0
np.fill_diagonal(A, 0)

A
```

```python
array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       ...,
       [0., 0., 0., ..., 0., 1., 1.],
       [0., 0., 1., ..., 1., 0., 1.],
       [0., 0., 0., ..., 1., 1., 0.]])
```



# Part B

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $i$th row-sum of $$\mathbf{A}$$, which is also called the *degree* of $$i$$. Let $$C_0$$ and $$C_1$$ be two clusters of the data points. We assume that every data point is in either $$C_0$$ or $$C_1$$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.  

The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

A pair of clusters $$C_0$$ and $$C_1$$ is considered to be a "good" partition of the data when $$N_{\mathbf{A}}(C_0, C_1)$$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$. Saying that this term should be small is the same as saying that points in $$C_0$$ shouldn't usually be very close to points in $$C_1$$. 

The formula is:

    $$
    \operatorname{cut}\left(C_{0}, C_{1}\right) \equiv \sum_{i \in C_{0}, j \in C_{1}} A_{i j}
    $$
Here is the function `cut(A, y)` based on the formula:

```python
def cut(A,y):
    total_cut = 0
    
    
    for i in range(len(y)):
        for j in range(len(y)): 
            if y[i] == 0 and y[j] == 1: #
                total_cut += A[i][j] # 
                
    return(total_cut)
```

Run this function:

```python
cut(A, y)
```

```python
13.0
```

Then, generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Check the cut objective for the random labels.

Let's make a random value first and then run the function:

```python
# make a random label between 0 and 1
random = np.random.randint(2, size = n)
cut(A, random)
```

```python
1150.0
```

By comparison, it is found that the cut objective for the true labels is *much* smaller than the cut objective for the random labels.

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 



#### B.2 The Volume Term 

In this part,  we need to get $$\operatorname{vol}(C 0)$$ and  $$\operatorname{vol}(C 1)$$ first. 

From above we know:

$$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 



So, the `vols(A, y)`  function is below:


```python
def vols(A, y):
    v0 = A[y == 0].sum()
    v1 = A[y == 1].sum()
    return(v0, v1)
```

Now, compare the `normcut` objective using both the true labels `y` and the fake labels you generated above. 

The `normcut(A, y)`  function is below:


```python
def normcut(A, y):
    v0, v1 = vols(A, y)
    norm_result = cut(A, y) * (1 / v0 + 1 / v1)
    return(norm_result)
```

Now, compare the `normcut`  objective using the true labels y and the random labels:


```python
normcut(A, y)
```


    0.011518412331615225


```python
normcut(A, random)
```


    1.0240023597759158

By comparison,  we can found that the `normcut`  of the true label is much smaller than the `normcut`  of the random label.



# Part C

Now, we have completed the `normcut(A, y)` function and the test results are as expected. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick!



Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^{n}$$ such that:


$$
z_{i}= \begin{cases}\frac{1}{\operatorname{vol}\left(C_{0}\right)} & \text { if } y_{i}=0 \\ -\frac{1}{\operatorname{vol}\left(C_{1}\right)} & \text { if } y_{i}=1\end{cases}
$$


Note that the signs of the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$ : if $$i$$ is in cluster $$C_{0}$$, then $$y_{i}=0$$ and $$z_{i}>0$$.



First, let us write a function `transform(A, y)` based on the  formula.

```python
ef transform(A, y):

    # get the volumes of clusters
    v0, v1 = vols(A, y)
    
    # computing the vector z
    z = np.zeros(n)
    z[y == 0] = 1 / v0
    z[y == 1] = -1 / v1
    return z
```



Next, we need to use `transform(A, y)` function to built this formula.


$$
\mathbf{N}_{\mathbf{A}}\left(C_{0}, C_{1}\right)=\frac{\mathbf{z}^{T}(\mathbf{D}-\mathbf{A}) \mathbf{z}}{\mathbf{z}^{T} \mathbf{D} \mathbf{z}}
$$


where $\mathbf{D}$ is the diagonal matrix with nonzero entries $$d_{i i}=d_{i}$$, and where $$d_{i}=\sum_{j=1}^{n} a_{i}$$ is the degree (row-sum) from before.

Note: compute $$\mathbf{z}^T\mathbf{D}\mathbf{z}$$ as `z@D@z`, provided that you have constructed these objects correctly. 

The function is below:

```python
# vector z
z = transform(A, y)
# diagonal matrix with nonzero entries
D = np.diag(sum(A))
# bulit the formula
formula = (z @ (D-A) @ z) / (z @ D @ z)
```

`np.isclose(a,b)` is a good way to check if `a` is "close" to `b`, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify. 

Then, check the equation above that relates the matrix product to the normcut objective.

```python
np.isclose(normcut(A, y), formula)
```

```python
True
```

check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`).

```python
# bulit the left side of this formula
left = z.T @ D @ np.ones(n)
# check if this identity holds true
np.isclose(left, 0)
```

```python
True
```



# Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 



$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$



subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$.



In the code below, it's help us to use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$.

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

The code in here:

```python
from scipy.optimize import minimize
# extracting the minimum vecctor
res = minimize(orth_obj, np.ones(len(z)))
z_min = res.x
```



# Part E

Recall that, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. 



Then, let's plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

```python
# z_min[i] < 0 will be 1; z_min[i] >= 0 will be 0
labels = np.where(z_min < 0, 1, 0)
plt.scatter(X[:,0], X[:,1], c = labels)
```

ÊèíÂÖ•ÂõæÁâá



# Part F

The problem in part E can actually be solved using the eigenvalues and eigenvectors of the matrix.



Recall that what we would like to do is minimize the function 



$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$



with respect to $\mathbf{z}$, subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 



$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$



which is equivalent to the standard eigenvalue problem 



$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$



Why is this helpful? Well, $$\mathbb{1}$$  is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 



> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with the *second*-smallest eigenvalue. 
>
> 

Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. 

Now, We will find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. 

```python
# construct the normalized Laplacian matrix
L = np.linalg.inv(D) @ (D - A)

# extract L eigenvalues and corresponding eigenvectors. 
eigenvalues, eigenvectors = np.linalg.eig(L)

# Sort the eigenvectors
sort = np.argsort(eigenvalues)
sort_eigenvectors = eigenvectors[:, sort]

# definition z_eig
z_eig = sort_eigenvectors[:, 1] 
```

Then, plot the data again, using the sign of `z_eig` as the color.

```python
# z_eig[i] < 0 will be 1; z_eig[i] >= 0 will be 0
labels2 = np.where(z_eig < 0, 1, 0)
plt.scatter(X[:,0], X[:,1], c = labels2)
```

