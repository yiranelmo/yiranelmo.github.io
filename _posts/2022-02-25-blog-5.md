---
layout: post
title: Blog Post 5 - Image Classification
---

In this blog post, we will learn several new skills and concepts related to image classification in Tensorflow.

- Tensorflow `Datasets` provide a convenient way for us to organize operations on our training, validation, and test data sets.
- *Data augmentation* allows us to create expanded versions of our data sets that allow models to learn patterns more robustly.
- *Transfer learning* allows us to use pre-trained models for new tasks.

Working on the coding portion of the Blog Post in Google Colab is strongly recommended. When training your model, enabling a GPU runtime (under Runtime -> Change Runtime Type) is likely to lead to significant speed benefits.



# §1. Load Packages and Obtain Data



We will load any packages we need in this code block.

```python
import os
import tensorflow as tf
from tensorflow.keras import utils
from matplotlib import pyplot as plt
import numpy as np
import random
from tensorflow.keras import datasets, layers, models
```



Now, let’s access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.

Paste and run the following code block.

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```



Paste the following code into the next block. This is the technical code related to reading data quickly. If you are interested in learning more about this kind of thing, you can take a look [here](https://www.tensorflow.org/guide/data_performance).

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```



## Working with Datasets



We can get a piece of a data set using the `take` method; e.g. `train_dataset.take(1)` will retrieve one batch (32 images with labels) from the training data.

Then, we will write a function to create a two-row visualization. For two-row visualization, the first row shows three random pictures of cats, and the second row shows three random pictures of dogs.

The function shows below:

```python
# size of the visualization
plt.figure(figsize=(10, 10))
class_names = ['cats', 'dogs']

def cat_dog():
  """
  The function to create a two-row visualization,
  and first row the image of cat, the second row is the image of dog.
  """
  
  for images, labels in train_dataset.take(1):
    i = 1
    # Using for-loop to draw cats visualization 
    for j in range(len(labels)):
      # check labels, 0 means cats
      if labels[j] == 0 and i < 4:
        ax = plt.subplot(2, 3, i)
        i = i + 1
        plt.imshow(images[j].numpy().astype("uint8"))
        plt.title(class_names[labels[j]])
        plt.axis("off")
        
    # Using for-loop to draw dogs visualization 
    for j in range(len(labels)):
      # check labels, 1 means dogs
      if labels[j] == 1 and i < 7:
        ax = plt.subplot(2, 3, i)
        i = i + 1
        plt.imshow(images[j].numpy().astype("uint8"))
        plt.title(class_names[labels[j]])
        plt.axis("off")

cat_dog()
```



![cat_dog.png](/images/cat_dog.png)



## Check Label Frequencies



The *baseline* machine learning model is the model that always guesses the most frequent label. So, we will get the baseline accuracy of our model in this part works.  



The following line of code will create an *iterator* called `labels`

```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```



As we know that `labels` equal to `0` means cats, and `labels` equal to `1` means dogs.  

Then, let's check the frequencies.

```python
sum(train_dataset.unbatch().map(lambda image, label: label == 0).as_numpy_iterator())
```

```python
1000
```

```python
sum(train_dataset.unbatch().map(lambda image, label: label == 1).as_numpy_iterator())
```

```python
1000
```



Based on the results of the above run we know that both the cat and the dog are 1000 and the sum is 2000. 

So, the **baseline accuracy** of our model is **50%**.



# §2. First Model



Create a `tf.keras.Sequential` model using some of the layers. For each model, include at least two `Conv2D` layers, at least two `MaxPooling2D` layers, at least one `Flatten` layer, at least one `Dense` layer, and at least one `Dropout` layer. 



For that part of the work, we refer to this [link](https://www.tensorflow.org/tutorials/images/cnn) to learn how to build `tf.keras.Sequential` mode.

For our first model, we need to make sure that we are able to consistently achieve **at least** 52% validation accuracy in this part (i.e. just a bit better than baseline).

 

Here is out first model. 

```python
# model 1
model1 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dropout(.5),
    layers.Dense(2)
])
```



Let’s train our model 1! 

**Note:**  training for 20 epochs with the `Dataset` settings described above should be sufficient.

```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 8s 83ms/step - loss: 5.5653 - accuracy: 0.5395 - val_loss: 0.8018 - val_accuracy: 0.5606
Epoch 2/20
63/63 [==============================] - 5s 75ms/step - loss: 0.7291 - accuracy: 0.5485 - val_loss: 0.7028 - val_accuracy: 0.5606
Epoch 3/20
63/63 [==============================] - 5s 75ms/step - loss: 0.6487 - accuracy: 0.6140 - val_loss: 0.6931 - val_accuracy: 0.5879
Epoch 4/20
63/63 [==============================] - 5s 76ms/step - loss: 0.5978 - accuracy: 0.6815 - val_loss: 0.7118 - val_accuracy: 0.5978
Epoch 5/20
63/63 [==============================] - 5s 74ms/step - loss: 0.5747 - accuracy: 0.6910 - val_loss: 0.7539 - val_accuracy: 0.6002
Epoch 6/20
63/63 [==============================] - 5s 75ms/step - loss: 0.5260 - accuracy: 0.7215 - val_loss: 0.7962 - val_accuracy: 0.6052
Epoch 7/20
63/63 [==============================] - 5s 76ms/step - loss: 0.4953 - accuracy: 0.7480 - val_loss: 0.7738 - val_accuracy: 0.6423
Epoch 8/20
63/63 [==============================] - 5s 74ms/step - loss: 0.4502 - accuracy: 0.7790 - val_loss: 0.7967 - val_accuracy: 0.6312
Epoch 9/20
63/63 [==============================] - 5s 74ms/step - loss: 0.4188 - accuracy: 0.8025 - val_loss: 0.8461 - val_accuracy: 0.6250
Epoch 10/20
63/63 [==============================] - 5s 75ms/step - loss: 0.3488 - accuracy: 0.8425 - val_loss: 0.8273 - val_accuracy: 0.6374
Epoch 11/20
63/63 [==============================] - 5s 76ms/step - loss: 0.3632 - accuracy: 0.8400 - val_loss: 0.8525 - val_accuracy: 0.6423
Epoch 12/20
63/63 [==============================] - 5s 76ms/step - loss: 0.3261 - accuracy: 0.8660 - val_loss: 0.9944 - val_accuracy: 0.6163
Epoch 13/20
63/63 [==============================] - 5s 76ms/step - loss: 0.3044 - accuracy: 0.8715 - val_loss: 1.0763 - val_accuracy: 0.6337
Epoch 14/20
63/63 [==============================] - 6s 95ms/step - loss: 0.2955 - accuracy: 0.8705 - val_loss: 1.0464 - val_accuracy: 0.6361
Epoch 15/20
63/63 [==============================] - 5s 77ms/step - loss: 0.2731 - accuracy: 0.8930 - val_loss: 1.0477 - val_accuracy: 0.6275
Epoch 16/20
63/63 [==============================] - 5s 76ms/step - loss: 0.2304 - accuracy: 0.9005 - val_loss: 1.0222 - val_accuracy: 0.6498
Epoch 17/20
63/63 [==============================] - 5s 76ms/step - loss: 0.2112 - accuracy: 0.9150 - val_loss: 1.1832 - val_accuracy: 0.6646
Epoch 18/20
63/63 [==============================] - 5s 75ms/step - loss: 0.2055 - accuracy: 0.9145 - val_loss: 1.1399 - val_accuracy: 0.6337
Epoch 19/20
63/63 [==============================] - 5s 75ms/step - loss: 0.1785 - accuracy: 0.9270 - val_loss: 1.1908 - val_accuracy: 0.6485
Epoch 20/20
63/63 [==============================] - 6s 83ms/step - loss: 0.1720 - accuracy: 0.9305 - val_loss: 1.2649 - val_accuracy: 0.6658
```



Let’s plot the history of the accuracy.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.52, color='black', label='Minimum accuracy = 52%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model1.png](/images/model1.png)



>
>
>**The validation accuracy of model stabilized between 56% and 67% during training.**
>
>**The baseline accuracy is 50%. And the validation accuracy is bigger than baseline accuracy.**
>
>**According to the plot, our training accuracy is much higher than the validation accuracy, so our model 1 is overfitting.**
>
>



# §3. Model with Data Augmentation



```python
# model 2
model2 = tf.keras.Sequential([
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),

    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(.2),
    layers.Dense(2),
])
```



Let’s train our model 2!

```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 7s 81ms/step - loss: 11.1415 - accuracy: 0.5145 - val_loss: 0.6824 - val_accuracy: 0.5953
Epoch 2/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6833 - accuracy: 0.5420 - val_loss: 0.6680 - val_accuracy: 0.6250
Epoch 3/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6762 - accuracy: 0.5980 - val_loss: 0.6759 - val_accuracy: 0.5866
Epoch 4/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6613 - accuracy: 0.6190 - val_loss: 0.6602 - val_accuracy: 0.6151
Epoch 5/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6541 - accuracy: 0.6365 - val_loss: 0.6394 - val_accuracy: 0.6361
Epoch 6/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6548 - accuracy: 0.6115 - val_loss: 0.6442 - val_accuracy: 0.6386
Epoch 7/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6520 - accuracy: 0.6215 - val_loss: 0.6261 - val_accuracy: 0.6634
Epoch 8/20
63/63 [==============================] - 5s 76ms/step - loss: 0.6375 - accuracy: 0.6445 - val_loss: 0.6300 - val_accuracy: 0.6200
Epoch 9/20
63/63 [==============================] - 5s 77ms/step - loss: 0.6243 - accuracy: 0.6595 - val_loss: 0.6355 - val_accuracy: 0.6250
Epoch 10/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6200 - accuracy: 0.6510 - val_loss: 0.6241 - val_accuracy: 0.6436
Epoch 11/20
63/63 [==============================] - 5s 77ms/step - loss: 0.6265 - accuracy: 0.6550 - val_loss: 0.6095 - val_accuracy: 0.6535
Epoch 12/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6205 - accuracy: 0.6690 - val_loss: 0.5884 - val_accuracy: 0.6980
Epoch 13/20
63/63 [==============================] - 5s 79ms/step - loss: 0.5998 - accuracy: 0.6895 - val_loss: 0.5873 - val_accuracy: 0.6918
Epoch 14/20
63/63 [==============================] - 6s 85ms/step - loss: 0.5978 - accuracy: 0.6725 - val_loss: 0.5856 - val_accuracy: 0.7054
Epoch 15/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5924 - accuracy: 0.6850 - val_loss: 0.5617 - val_accuracy: 0.7153
Epoch 16/20
63/63 [==============================] - 5s 79ms/step - loss: 0.5748 - accuracy: 0.7005 - val_loss: 0.5766 - val_accuracy: 0.7067
Epoch 17/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5748 - accuracy: 0.6940 - val_loss: 0.5605 - val_accuracy: 0.7005
Epoch 18/20
63/63 [==============================] - 5s 79ms/step - loss: 0.5984 - accuracy: 0.6740 - val_loss: 0.5834 - val_accuracy: 0.7067
Epoch 19/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5852 - accuracy: 0.6890 - val_loss: 0.5804 - val_accuracy: 0.6980
Epoch 20/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5691 - accuracy: 0.6935 - val_loss: 0.5629 - val_accuracy: 0.7104
```



Let’s plot the history of the accuracy.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.55, color='black', label='Minimum accuracy = 55%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model2.png](/images/model2.png)





>
>
>**The validation accuracy of model stabilized between 58% and 72% during training.**
>
>



# §4. Data Preprocessing



```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```



```python
# model 3
model3 = models.Sequential([
    # Preprocessor
    preprocessor,
    # Data Augmentation
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(2)
])
```



Let’s train our model 3!

```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 7s 87ms/step - loss: 0.5815 - accuracy: 0.6990 - val_loss: 0.5890 - val_accuracy: 0.6819
Epoch 2/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5637 - accuracy: 0.7050 - val_loss: 0.5667 - val_accuracy: 0.7116
Epoch 3/20
63/63 [==============================] - 6s 91ms/step - loss: 0.5559 - accuracy: 0.7150 - val_loss: 0.5737 - val_accuracy: 0.7005
Epoch 4/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5481 - accuracy: 0.7245 - val_loss: 0.5589 - val_accuracy: 0.7302
Epoch 5/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5431 - accuracy: 0.7220 - val_loss: 0.5813 - val_accuracy: 0.7141
Epoch 6/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5378 - accuracy: 0.7205 - val_loss: 0.5573 - val_accuracy: 0.7215
Epoch 7/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5451 - accuracy: 0.7275 - val_loss: 0.5864 - val_accuracy: 0.6894
Epoch 8/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5434 - accuracy: 0.7225 - val_loss: 0.5405 - val_accuracy: 0.7290
Epoch 9/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5315 - accuracy: 0.7415 - val_loss: 0.5562 - val_accuracy: 0.7104
Epoch 10/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5198 - accuracy: 0.7510 - val_loss: 0.5616 - val_accuracy: 0.7252
Epoch 11/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5165 - accuracy: 0.7420 - val_loss: 0.5442 - val_accuracy: 0.7265
Epoch 12/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5016 - accuracy: 0.7465 - val_loss: 0.6034 - val_accuracy: 0.6745
Epoch 13/20
63/63 [==============================] - 5s 83ms/step - loss: 0.5165 - accuracy: 0.7435 - val_loss: 0.5342 - val_accuracy: 0.7401
Epoch 14/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5261 - accuracy: 0.7410 - val_loss: 0.5668 - val_accuracy: 0.7067
Epoch 15/20
63/63 [==============================] - 6s 99ms/step - loss: 0.5023 - accuracy: 0.7530 - val_loss: 0.5226 - val_accuracy: 0.7475
Epoch 16/20
63/63 [==============================] - 6s 84ms/step - loss: 0.4919 - accuracy: 0.7575 - val_loss: 0.5195 - val_accuracy: 0.7488
Epoch 17/20
63/63 [==============================] - 6s 83ms/step - loss: 0.5011 - accuracy: 0.7720 - val_loss: 0.5493 - val_accuracy: 0.6993
Epoch 18/20
63/63 [==============================] - 6s 84ms/step - loss: 0.5026 - accuracy: 0.7545 - val_loss: 0.5350 - val_accuracy: 0.7228
Epoch 19/20
63/63 [==============================] - 5s 82ms/step - loss: 0.4845 - accuracy: 0.7645 - val_loss: 0.5373 - val_accuracy: 0.7364
Epoch 20/20
63/63 [==============================] - 6s 85ms/step - loss: 0.4877 - accuracy: 0.7630 - val_loss: 0.5774 - val_accuracy: 0.7017
```



Let’s plot the history of the accuracy.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.70, color='black', label='Minimum accuracy = 70%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model3.png](/images/model3.png)





>
>
>**The validation accuracy of model stabilized between 67% and 75% during training.**
>
>



# §5. Transfer Learning



```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```



```python
# model 4
model4 = tf.keras.Sequential([
    # Preprocessor
    preprocessor,
    # Data Augmentation
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),

    # Base model 
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.5),
    layers.Dense(2)
])
```



Let’s train our model 4!

```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 13s 113ms/step - loss: 2.0201 - accuracy: 0.6795 - val_loss: 0.1557 - val_accuracy: 0.9455
Epoch 2/20
63/63 [==============================] - 6s 87ms/step - loss: 0.9871 - accuracy: 0.8160 - val_loss: 0.1272 - val_accuracy: 0.9592
Epoch 3/20
63/63 [==============================] - 6s 88ms/step - loss: 0.7142 - accuracy: 0.8525 - val_loss: 0.1108 - val_accuracy: 0.9678
Epoch 4/20
63/63 [==============================] - 6s 87ms/step - loss: 0.6595 - accuracy: 0.8605 - val_loss: 0.1047 - val_accuracy: 0.9678
Epoch 5/20
63/63 [==============================] - 6s 86ms/step - loss: 0.6864 - accuracy: 0.8580 - val_loss: 0.0923 - val_accuracy: 0.9691
Epoch 6/20
63/63 [==============================] - 6s 85ms/step - loss: 0.5790 - accuracy: 0.8690 - val_loss: 0.0984 - val_accuracy: 0.9691
Epoch 7/20
63/63 [==============================] - 6s 87ms/step - loss: 0.5817 - accuracy: 0.8620 - val_loss: 0.0837 - val_accuracy: 0.9752
Epoch 8/20
63/63 [==============================] - 6s 87ms/step - loss: 0.5659 - accuracy: 0.8710 - val_loss: 0.1058 - val_accuracy: 0.9691
Epoch 9/20
63/63 [==============================] - 6s 93ms/step - loss: 0.4454 - accuracy: 0.8820 - val_loss: 0.0950 - val_accuracy: 0.9653
Epoch 10/20
63/63 [==============================] - 6s 92ms/step - loss: 0.4804 - accuracy: 0.8845 - val_loss: 0.0858 - val_accuracy: 0.9703
Epoch 11/20
63/63 [==============================] - 6s 86ms/step - loss: 0.4096 - accuracy: 0.8840 - val_loss: 0.0839 - val_accuracy: 0.9691
Epoch 12/20
63/63 [==============================] - 6s 86ms/step - loss: 0.5232 - accuracy: 0.8770 - val_loss: 0.1131 - val_accuracy: 0.9691
Epoch 13/20
63/63 [==============================] - 6s 87ms/step - loss: 0.4424 - accuracy: 0.8805 - val_loss: 0.0736 - val_accuracy: 0.9752
Epoch 14/20
63/63 [==============================] - 6s 86ms/step - loss: 0.3821 - accuracy: 0.8910 - val_loss: 0.1000 - val_accuracy: 0.9691
Epoch 15/20
63/63 [==============================] - 6s 86ms/step - loss: 0.3850 - accuracy: 0.9000 - val_loss: 0.0697 - val_accuracy: 0.9715
Epoch 16/20
63/63 [==============================] - 6s 86ms/step - loss: 0.4115 - accuracy: 0.8800 - val_loss: 0.0777 - val_accuracy: 0.9703
Epoch 17/20
63/63 [==============================] - 6s 87ms/step - loss: 0.3725 - accuracy: 0.8935 - val_loss: 0.0901 - val_accuracy: 0.9703
Epoch 18/20
63/63 [==============================] - 6s 87ms/step - loss: 0.3937 - accuracy: 0.8975 - val_loss: 0.0797 - val_accuracy: 0.9703
Epoch 19/20
63/63 [==============================] - 6s 86ms/step - loss: 0.3618 - accuracy: 0.8930 - val_loss: 0.0801 - val_accuracy: 0.9715
Epoch 20/20
63/63 [==============================] - 6s 87ms/step - loss: 0.3643 - accuracy: 0.8990 - val_loss: 0.0847 - val_accuracy: 0.9703
```



Let’s plot the history of the accuracy.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.90, color='black', label='Minimum accuracy = 90%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model4.png](/images/model4.png)





>
>
>**The validation accuracy of model stabilized between 94% and 98% during training.**
>
>



# §6. Score on Test Data



Finally, let's evaluate `model 4` as it is the best performing model on the unseen ` test_dataset`.

```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```



The output below:

```python
6/6 [==============================] - 1s 54ms/step - loss: 0.0889 - accuracy: 0.9635
Test accuracy : 0.9635416865348816
```





