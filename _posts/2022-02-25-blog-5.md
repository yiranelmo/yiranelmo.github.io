---
layout: post
title: Blog Post 5 - Image Classification
---

In this blog post, we will learn several new skills and concepts related to image classification in Tensorflow.

- Tensorflow `Datasets` provide a convenient way for us to organize operations on our training, validation, and test data sets.
- *Data augmentation* allows us to create expanded versions of our data sets that allow models to learn patterns more robustly.
- *Transfer learning* allows us to use pre-trained models for new tasks.

Working on the coding portion of the Blog Post in Google Colab is strongly recommended. When training your model, enabling a GPU runtime (under Runtime -> Change Runtime Type) is likely to lead to significant speed benefits.



# §1. Load Packages and Obtain Data



We will load any packages we need in this code block.

```python
import os
import tensorflow as tf
from tensorflow.keras import utils
from matplotlib import pyplot as plt
import numpy as np
import random
from tensorflow.keras import datasets, layers, models
```



Now, let’s access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.

Paste and run the following code block.

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

Paste the following code into the next block. This is the technical code related to reading data quickly. If you are interested in learning more about this kind of thing, you can take a look [here](https://www.tensorflow.org/guide/data_performance).

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```



## Working with Datasets



```python
plt.figure(figsize=(10, 10))
class_names = ['cats', 'dogs']

def cat_dog():
  for images, labels in train_dataset.take(1):
    i = 1
    for j in range(len(labels)):
      if labels[j] == 0 and i < 4:
        ax = plt.subplot(2, 3, i)
        i = i + 1
        plt.imshow(images[j].numpy().astype("uint8"))
        plt.title(class_names[labels[j]])
        plt.axis("off")
    for j in range(len(labels)):
      if labels[j] == 1 and i < 7:
        ax = plt.subplot(2, 3, i)
        i = i + 1
        plt.imshow(images[j].numpy().astype("uint8"))
        plt.title(class_names[labels[j]])
        plt.axis("off")

cat_dog()
```



插入图片。



## Check Label Frequencies



```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

```python
dog = 0
cat = 0
for i in labels_iterator:
  if i == 0:
    cat = cat + 1
  else:
    dog = dog + 1
print(cat, dog)
```

插入结果



# §2. First Model



```python
# model 1
model1 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),

    layers.Dropout(.5),
    layers.Dense(2)
])
```



```path
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 8s 83ms/step - loss: 5.5653 - accuracy: 0.5395 - val_loss: 0.8018 - val_accuracy: 0.5606
Epoch 2/20
63/63 [==============================] - 5s 75ms/step - loss: 0.7291 - accuracy: 0.5485 - val_loss: 0.7028 - val_accuracy: 0.5606
Epoch 3/20
63/63 [==============================] - 5s 75ms/step - loss: 0.6487 - accuracy: 0.6140 - val_loss: 0.6931 - val_accuracy: 0.5879
Epoch 4/20
63/63 [==============================] - 5s 76ms/step - loss: 0.5978 - accuracy: 0.6815 - val_loss: 0.7118 - val_accuracy: 0.5978
Epoch 5/20
63/63 [==============================] - 5s 74ms/step - loss: 0.5747 - accuracy: 0.6910 - val_loss: 0.7539 - val_accuracy: 0.6002
Epoch 6/20
63/63 [==============================] - 5s 75ms/step - loss: 0.5260 - accuracy: 0.7215 - val_loss: 0.7962 - val_accuracy: 0.6052
Epoch 7/20
63/63 [==============================] - 5s 76ms/step - loss: 0.4953 - accuracy: 0.7480 - val_loss: 0.7738 - val_accuracy: 0.6423
Epoch 8/20
63/63 [==============================] - 5s 74ms/step - loss: 0.4502 - accuracy: 0.7790 - val_loss: 0.7967 - val_accuracy: 0.6312
Epoch 9/20
63/63 [==============================] - 5s 74ms/step - loss: 0.4188 - accuracy: 0.8025 - val_loss: 0.8461 - val_accuracy: 0.6250
Epoch 10/20
63/63 [==============================] - 5s 75ms/step - loss: 0.3488 - accuracy: 0.8425 - val_loss: 0.8273 - val_accuracy: 0.6374
Epoch 11/20
63/63 [==============================] - 5s 76ms/step - loss: 0.3632 - accuracy: 0.8400 - val_loss: 0.8525 - val_accuracy: 0.6423
Epoch 12/20
63/63 [==============================] - 5s 76ms/step - loss: 0.3261 - accuracy: 0.8660 - val_loss: 0.9944 - val_accuracy: 0.6163
Epoch 13/20
63/63 [==============================] - 5s 76ms/step - loss: 0.3044 - accuracy: 0.8715 - val_loss: 1.0763 - val_accuracy: 0.6337
Epoch 14/20
63/63 [==============================] - 6s 95ms/step - loss: 0.2955 - accuracy: 0.8705 - val_loss: 1.0464 - val_accuracy: 0.6361
Epoch 15/20
63/63 [==============================] - 5s 77ms/step - loss: 0.2731 - accuracy: 0.8930 - val_loss: 1.0477 - val_accuracy: 0.6275
Epoch 16/20
63/63 [==============================] - 5s 76ms/step - loss: 0.2304 - accuracy: 0.9005 - val_loss: 1.0222 - val_accuracy: 0.6498
Epoch 17/20
63/63 [==============================] - 5s 76ms/step - loss: 0.2112 - accuracy: 0.9150 - val_loss: 1.1832 - val_accuracy: 0.6646
Epoch 18/20
63/63 [==============================] - 5s 75ms/step - loss: 0.2055 - accuracy: 0.9145 - val_loss: 1.1399 - val_accuracy: 0.6337
Epoch 19/20
63/63 [==============================] - 5s 75ms/step - loss: 0.1785 - accuracy: 0.9270 - val_loss: 1.1908 - val_accuracy: 0.6485
Epoch 20/20
63/63 [==============================] - 6s 83ms/step - loss: 0.1720 - accuracy: 0.9305 - val_loss: 1.2649 - val_accuracy: 0.6658
```





```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.52, color='black', label='Minimum accuracy = 52%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model1.png](/images/model1.png)





>
>
>**The validation accuracy of model stabilized between 56% and 67% during training.**
>
>



# §3. Model with Data Augmentation



```python
# model 2
model2 = tf.keras.Sequential([
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),

    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(.2),
    layers.Dense(2),
])
```



```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 7s 81ms/step - loss: 11.1415 - accuracy: 0.5145 - val_loss: 0.6824 - val_accuracy: 0.5953
Epoch 2/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6833 - accuracy: 0.5420 - val_loss: 0.6680 - val_accuracy: 0.6250
Epoch 3/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6762 - accuracy: 0.5980 - val_loss: 0.6759 - val_accuracy: 0.5866
Epoch 4/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6613 - accuracy: 0.6190 - val_loss: 0.6602 - val_accuracy: 0.6151
Epoch 5/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6541 - accuracy: 0.6365 - val_loss: 0.6394 - val_accuracy: 0.6361
Epoch 6/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6548 - accuracy: 0.6115 - val_loss: 0.6442 - val_accuracy: 0.6386
Epoch 7/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6520 - accuracy: 0.6215 - val_loss: 0.6261 - val_accuracy: 0.6634
Epoch 8/20
63/63 [==============================] - 5s 76ms/step - loss: 0.6375 - accuracy: 0.6445 - val_loss: 0.6300 - val_accuracy: 0.6200
Epoch 9/20
63/63 [==============================] - 5s 77ms/step - loss: 0.6243 - accuracy: 0.6595 - val_loss: 0.6355 - val_accuracy: 0.6250
Epoch 10/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6200 - accuracy: 0.6510 - val_loss: 0.6241 - val_accuracy: 0.6436
Epoch 11/20
63/63 [==============================] - 5s 77ms/step - loss: 0.6265 - accuracy: 0.6550 - val_loss: 0.6095 - val_accuracy: 0.6535
Epoch 12/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6205 - accuracy: 0.6690 - val_loss: 0.5884 - val_accuracy: 0.6980
Epoch 13/20
63/63 [==============================] - 5s 79ms/step - loss: 0.5998 - accuracy: 0.6895 - val_loss: 0.5873 - val_accuracy: 0.6918
Epoch 14/20
63/63 [==============================] - 6s 85ms/step - loss: 0.5978 - accuracy: 0.6725 - val_loss: 0.5856 - val_accuracy: 0.7054
Epoch 15/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5924 - accuracy: 0.6850 - val_loss: 0.5617 - val_accuracy: 0.7153
Epoch 16/20
63/63 [==============================] - 5s 79ms/step - loss: 0.5748 - accuracy: 0.7005 - val_loss: 0.5766 - val_accuracy: 0.7067
Epoch 17/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5748 - accuracy: 0.6940 - val_loss: 0.5605 - val_accuracy: 0.7005
Epoch 18/20
63/63 [==============================] - 5s 79ms/step - loss: 0.5984 - accuracy: 0.6740 - val_loss: 0.5834 - val_accuracy: 0.7067
Epoch 19/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5852 - accuracy: 0.6890 - val_loss: 0.5804 - val_accuracy: 0.6980
Epoch 20/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5691 - accuracy: 0.6935 - val_loss: 0.5629 - val_accuracy: 0.7104
```



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.55, color='black', label='Minimum accuracy = 55%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model2.png](/images/model2.png)





>
>
>**The validation accuracy of model stabilized between 58% and 72% during training.**
>
>



# §4. Data Preprocessing



```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```



```python
# model 3
model3 = models.Sequential([
    # Preprocessor
    preprocessor,
    # Data Augmentation
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(2)
])
```



```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 7s 87ms/step - loss: 0.5815 - accuracy: 0.6990 - val_loss: 0.5890 - val_accuracy: 0.6819
Epoch 2/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5637 - accuracy: 0.7050 - val_loss: 0.5667 - val_accuracy: 0.7116
Epoch 3/20
63/63 [==============================] - 6s 91ms/step - loss: 0.5559 - accuracy: 0.7150 - val_loss: 0.5737 - val_accuracy: 0.7005
Epoch 4/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5481 - accuracy: 0.7245 - val_loss: 0.5589 - val_accuracy: 0.7302
Epoch 5/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5431 - accuracy: 0.7220 - val_loss: 0.5813 - val_accuracy: 0.7141
Epoch 6/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5378 - accuracy: 0.7205 - val_loss: 0.5573 - val_accuracy: 0.7215
Epoch 7/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5451 - accuracy: 0.7275 - val_loss: 0.5864 - val_accuracy: 0.6894
Epoch 8/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5434 - accuracy: 0.7225 - val_loss: 0.5405 - val_accuracy: 0.7290
Epoch 9/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5315 - accuracy: 0.7415 - val_loss: 0.5562 - val_accuracy: 0.7104
Epoch 10/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5198 - accuracy: 0.7510 - val_loss: 0.5616 - val_accuracy: 0.7252
Epoch 11/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5165 - accuracy: 0.7420 - val_loss: 0.5442 - val_accuracy: 0.7265
Epoch 12/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5016 - accuracy: 0.7465 - val_loss: 0.6034 - val_accuracy: 0.6745
Epoch 13/20
63/63 [==============================] - 5s 83ms/step - loss: 0.5165 - accuracy: 0.7435 - val_loss: 0.5342 - val_accuracy: 0.7401
Epoch 14/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5261 - accuracy: 0.7410 - val_loss: 0.5668 - val_accuracy: 0.7067
Epoch 15/20
63/63 [==============================] - 6s 99ms/step - loss: 0.5023 - accuracy: 0.7530 - val_loss: 0.5226 - val_accuracy: 0.7475
Epoch 16/20
63/63 [==============================] - 6s 84ms/step - loss: 0.4919 - accuracy: 0.7575 - val_loss: 0.5195 - val_accuracy: 0.7488
Epoch 17/20
63/63 [==============================] - 6s 83ms/step - loss: 0.5011 - accuracy: 0.7720 - val_loss: 0.5493 - val_accuracy: 0.6993
Epoch 18/20
63/63 [==============================] - 6s 84ms/step - loss: 0.5026 - accuracy: 0.7545 - val_loss: 0.5350 - val_accuracy: 0.7228
Epoch 19/20
63/63 [==============================] - 5s 82ms/step - loss: 0.4845 - accuracy: 0.7645 - val_loss: 0.5373 - val_accuracy: 0.7364
Epoch 20/20
63/63 [==============================] - 6s 85ms/step - loss: 0.4877 - accuracy: 0.7630 - val_loss: 0.5774 - val_accuracy: 0.7017
```



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.70, color='black', label='Minimum accuracy = 70%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model3.png](/images/model3.png)





>
>
>**The validation accuracy of model stabilized between 67% and 75% during training.**
>
>



# §5. Transfer Learning



```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```



```python
# model 4
model4 = tf.keras.Sequential([
    # Preprocessor
    preprocessor,
    # Data Augmentation
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),

    # Base model 
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.5),
    layers.Dense(2)
])
```



```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```



The output below:

```python
Epoch 1/20
63/63 [==============================] - 13s 113ms/step - loss: 1.5629 - accuracy: 0.7215 - val_loss: 0.2175 - val_accuracy: 0.9356
Epoch 2/20
63/63 [==============================] - 6s 87ms/step - loss: 0.9056 - accuracy: 0.8370 - val_loss: 0.1738 - val_accuracy: 0.9505
Epoch 3/20
63/63 [==============================] - 6s 87ms/step - loss: 0.7643 - accuracy: 0.8465 - val_loss: 0.1033 - val_accuracy: 0.9691
Epoch 4/20
63/63 [==============================] - 6s 88ms/step - loss: 0.6723 - accuracy: 0.8670 - val_loss: 0.1046 - val_accuracy: 0.9715
Epoch 5/20
63/63 [==============================] - 6s 87ms/step - loss: 0.6424 - accuracy: 0.8670 - val_loss: 0.2221 - val_accuracy: 0.9319
Epoch 6/20
63/63 [==============================] - 6s 88ms/step - loss: 0.5946 - accuracy: 0.8685 - val_loss: 0.1806 - val_accuracy: 0.9443
Epoch 7/20
63/63 [==============================] - 6s 88ms/step - loss: 0.5383 - accuracy: 0.8755 - val_loss: 0.0983 - val_accuracy: 0.9715
Epoch 8/20
63/63 [==============================] - 6s 87ms/step - loss: 0.5380 - accuracy: 0.8775 - val_loss: 0.1612 - val_accuracy: 0.9530
Epoch 9/20
63/63 [==============================] - 6s 88ms/step - loss: 0.5307 - accuracy: 0.8830 - val_loss: 0.1046 - val_accuracy: 0.9666
Epoch 10/20
63/63 [==============================] - 6s 88ms/step - loss: 0.5534 - accuracy: 0.8875 - val_loss: 0.0872 - val_accuracy: 0.9740
Epoch 11/20
63/63 [==============================] - 6s 87ms/step - loss: 0.4472 - accuracy: 0.8990 - val_loss: 0.0837 - val_accuracy: 0.9728
Epoch 12/20
63/63 [==============================] - 6s 91ms/step - loss: 0.5088 - accuracy: 0.8795 - val_loss: 0.0764 - val_accuracy: 0.9752
Epoch 13/20
63/63 [==============================] - 6s 89ms/step - loss: 0.4150 - accuracy: 0.8930 - val_loss: 0.0932 - val_accuracy: 0.9641
Epoch 14/20
63/63 [==============================] - 6s 88ms/step - loss: 0.4082 - accuracy: 0.8960 - val_loss: 0.0735 - val_accuracy: 0.9728
Epoch 15/20
63/63 [==============================] - 6s 90ms/step - loss: 0.4344 - accuracy: 0.8780 - val_loss: 0.0871 - val_accuracy: 0.9703
Epoch 16/20
63/63 [==============================] - 6s 92ms/step - loss: 0.4230 - accuracy: 0.8890 - val_loss: 0.0690 - val_accuracy: 0.9777
Epoch 17/20
63/63 [==============================] - 6s 89ms/step - loss: 0.3816 - accuracy: 0.8990 - val_loss: 0.0690 - val_accuracy: 0.9765
Epoch 18/20
63/63 [==============================] - 6s 87ms/step - loss: 0.3658 - accuracy: 0.8895 - val_loss: 0.0651 - val_accuracy: 0.9752
Epoch 19/20
63/63 [==============================] - 6s 87ms/step - loss: 0.3650 - accuracy: 0.8965 - val_loss: 0.0621 - val_accuracy: 0.9765
Epoch 20/20
63/63 [==============================] - 6s 89ms/step - loss: 0.4018 - accuracy: 0.8910 - val_loss: 0.1363 - val_accuracy: 0.9505
```



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.ylim([0,1.1])
plt.axhline(y=0.90, color='black', label='Minimum accuracy = 90%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model4.png](/images/model4.png)





>
>
>**The validation accuracy of model stabilized between 95% and 98% during training.**
>
>



# §6. Score on Test Data



Finally, let's evaluate `model 4` as it is the best performing model on the unseen ` test_dataset`.

```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```



The output below:

```python
6/6 [==============================] - 1s 64ms/step - loss: 0.1253 - accuracy: 0.9688
Test accuracy : 0.96875
```





