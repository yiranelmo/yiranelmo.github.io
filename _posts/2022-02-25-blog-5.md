---
layout: post
title: Blog Post 5 - Image Classification
---

In this blog post, we will learn several new skills and concepts related to image classification in Tensorflow.

- Tensorflow `Datasets` provide a convenient way for us to organize operations on our training, validation, and test data sets.
- *Data augmentation* allows us to create expanded versions of our data sets that allow models to learn patterns more robustly.
- *Transfer learning* allows us to use pre-trained models for new tasks.

Working on the coding portion of the Blog Post in Google Colab is strongly recommended. When training your model, enabling a GPU runtime (under Runtime -> Change Runtime Type) is likely to lead to significant speed benefits.



# Load Packages and Obtain Data



We will load any packages we need in this code block.

```python
import os
import tensorflow as tf
from tensorflow.keras import utils
from matplotlib import pyplot as plt
import numpy as np
import random
from tensorflow.keras import datasets, layers, models
```



Now, let’s access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.

Paste and run the following code block.

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

Paste the following code into the next block. This is the technical code related to reading data quickly. If you are interested in learning more about this kind of thing, you can take a look [here](https://www.tensorflow.org/guide/data_performance).

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```





## Working with Datasets



```python
plt.figure(figsize=(10, 10))
class_names = ['cats', 'dogs']

def cat_dog():
  for images, labels in train_dataset.take(1):
    i = 1
    for j in range(len(labels)):
      if labels[j] == 0 and i < 4:
        ax = plt.subplot(2, 3, i)
        i = i + 1
        plt.imshow(images[j].numpy().astype("uint8"))
        plt.title(class_names[labels[j]])
        plt.axis("off")
    for j in range(len(labels)):
      if labels[j] == 1 and i < 7:
        ax = plt.subplot(2, 3, i)
        i = i + 1
        plt.imshow(images[j].numpy().astype("uint8"))
        plt.title(class_names[labels[j]])
        plt.axis("off")

cat_dog()
```

![Intro_3.png](/images/Intro_3.png)



## Check Label Frequencies



```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

```python
dog = 0
cat = 0
for i in labels_iterator:
  if i == 0:
    cat = cat + 1
  else:
    dog = dog + 1
print(cat, dog)
```







>
>
>







