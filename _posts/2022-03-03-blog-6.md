---
layout: post
title: Blog Post 6 - Fake News Classification
---

In this Blog Post, you will develop and assess a fake news classifier using Tensorflow.

**Note**: *Working on this Blog Post in Google Colab is highly recommended.*



**A tutorial for the project can be found at this [link](https://www.tensorflow.org/tutorials/images/transfer_learning).**

**This [link](https://colab.research.google.com/drive/1CuDc1MXkiOgTxZ-kdzRNn0hpaBBgKomu?usp=sharing) is the code of this project.**



# §1. Acquire Training Data



I have hosted a training data set at the below URL. We can either read it into Python directly (via `pd.read_csv()`) or download it to your computer and read it from disk.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```



Then let's us read this data.

```python
import pandas as pd
import numpy as np
df = pd.read_csv(train_url)
df.head()
```



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>


Each data row corresponds to a single article. The title column contains the article's title, while the text column has the whole article text. As established by the authors of the study above, the last column, labeled fake, is 0 if the article is real and 1 if it includes fake news.





# §2. Make a Dataset



In this part,  we need to write a function and it should do two things:

1. *Remove stopwords* from the article `text` and `title`. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find [this StackOverFlow thread](https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe) to be helpful.
2. Construct and return a `tf.data.Dataset` with two inputs and one output. The input should be of the form `(title, text)`, and the output should consist only of the `fake` column. You may find it helpful to consult [these lecture notes](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-4.ipynb) or [this tutorial](https://www.tensorflow.org/guide/-keras/functional) for reference on how to construct and use `Dataset`s with multiple inputs.



```python
# Load the required packages
import tensorflow as tf
from tensorflow import keras
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')


def make_dataset(df):
  
  # remove stopwords from text and title
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  
  # bulid tf dataset
  # the first dictionary is the two inputs 
  # the second dictionary is the output
  data = tf.data.Dataset.from_tensor_slices(
      ({
          "title" : df[["title"]],
          "text"  : df[["text"]]
      },
       {
           "fake" : df[["fake"]]
        }))
  # increase the speed of training
  data.batch(100)
  return data
```



### Validation Data



After we’ve constructed your primary `Dataset`, split of 20% of it to use for validation.



```python
data = make_dataset(df)

train_size = int(0.8*len(data))
val_size = int(0.2*len(data))

train = data.take(train_size).batch(100)
val = data.skip(train_size).take(val_size).batch(100)
```



### Base Rate



The base rate refers to the accuracy of a model that always makes the same guess.

In this part,  we need to determine the base rate for this data set by examining the labels on the training set.



```python
df['fake'].value_counts()
```



The output below 

```python
1    11740
0    10709
Name: fake, dtype: int64
```



Then,  let's get the base rate.

```python
11740/(11740+10709)
```



The output below

```python
0.522963160942581
```



So, the base rate is 52.3%.





# §3. Create Models



Now,  we will use TensorFlow models to offer a perspective on the following question:

> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?



To address this question, create **three (3)** TensorFlow models.

1. In the first model, you should use **only the article title** as an input.
2. In the second model, you should use **only the article text** as an input.
3. In the third model, you should use **both the article title and the article text** as input.



***standardization & vectorization***



Standardizations include remove capitals and  remove punctuation; vectorization represents text as a vector.

```python
import string
import re
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras import Input, layers, Model, utils, losses

size_vocabulary = 2000

# standardize text by removing capitals and punctuation
def standardization(input_data):
    
    # all letters to lowercase
    lowercase = tf.strings.lower(input_data)
    # remove punctuation
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation

# represent text as a vector
vectorize_layer = TextVectorization(
    # standardize the sample
    standardize = standardization,
    max_tokens = size_vocabulary,
    output_mode = 'int',
    output_sequence_length = 500) 
```



Then apply the vectorization layer to the training data's title and text

```python
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```



## Title  Model



The first model we'll build will use the title of an article to assess if it includes false news or not.



First job will define the input types of the data. Then, write the pipeline for title.

```python
# title model input
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

# embedding layer
embedding = layers.Embedding(size_vocabulary, 16, name = "embedding")

# pipeline for title
title_features = vectorize_layer(title_input)
title_features = embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

# title model output
output_title = layers.Dense(2, name = "fake")(title_features)
```



Here is the first model.

```python
model_title = keras.Model(
    inputs = [title_input],
    outputs = output_title
)
```



Let’s train our model 1! 

```python
model_title.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)

history_title = model_title.fit(train,
                    validation_data = val,
                    epochs = 20)
```



Let’s plot the history of the accuracy.

```python
from matplotlib import pyplot as plt
plt.plot(history_title.history["accuracy"], label = "training")
plt.plot(history_title.history["val_accuracy"], label = "validation")
plt.axhline(y = 0.97, color = 'black', label = 'Minimum accuracy = 97%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model_1.png](/images/model_1.png)



>
>
>**The validation accuracy of model stabilized 97% during training.**
>
>**The baseline accuracy is 52.3%. And the validation accuracy is bigger than baseline accuracy.**
>
>
>



## Text Model



The second  model we'll build will use the text of an article to assess if it includes false news or not.



First job will define the input types of the data. Then, write the pipeline for title.

```python
# text model input
text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)

# pipeline for title
text_features = vectorize_layer(text_input)
text_features = embedding(text_features)
text_features = layers.Dropout(0.75)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.5)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

# text model output
text_output = layers.Dense(2, name = "fake")(text_features)
```



Here is the second model.

```python
model_test = keras.Model(
    inputs = [text_input],
    outputs = text_output
)
```



Let’s train our model 2!

```python
model_test.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)

history_title = model_test.fit(train,
                    validation_data = val,
                    epochs = 20)
```



Let’s plot the history of the accuracy.

```python
plt.plot(history_title.history["accuracy"], label = "training")
plt.plot(history_title.history["val_accuracy"], label = "validation")
plt.axhline(y = 0.97, color = 'black', label = 'Minimum accuracy = 97%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model_2.png](/images/model_2.png)



>
>
>**The validation accuracy of the model can reach 97% during training. **
>
>



## Title & Text Model 



The third  model we'll build will use the text and title of an article to assess if it includes false news or not.



First we need to combine `title_features` and `text_features` to create our new model. Then dense layer and then create our output layer.

```python
combined_features = layers.concatenate([title_features, text_features], axis = 1)
combined_features = layers.Dense(32, activation = 'relu')(combined_features)
# model output
combined_output = layers.Dense(2, name = "fake")(combined_features)
```



Here is model 3!

```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = combined_output
)
```



Let’s train our model 3!

```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)
history_title = model.fit(train,
                    validation_data = val,
                    epochs = 20)
```



Let’s plot the history of the accuracy.

```python
plt.plot(history_title.history["accuracy"], label = "training")
plt.plot(history_title.history["val_accuracy"], label = "validation")
plt.axhline(y = 0.97, color = 'black', label = 'Minimum accuracy = 97%')
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```



![model_3.png](/images/model_3.png)



>
>
>**The validation accuracy of the model can stabilize above 97% during training. **
>
>



***According to the running results of these three models, when trying to detect fake news, algorithms should use both text and title.***





# §4. Model Evaluation



Since we know the best model is the model 3. 

For this part, you can focus on our **best** model, and ignore the other two.



Here is the test url.

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
```



Let's get the evaluate by using the third model.

```python
test_data = pd.read_csv(test_url)
test_dataset = make_dataset(test_data)
model.evaluate(test_dataset)
```



 The output below.

```python
22449/22449 [==============================] - 73s 3ms/step - loss: 0.0349 - accuracy: 0.9896
[0.034931790083646774, 0.9895763993263245]
```



**The validation accuracy is 98.96%.**  The result can show us that it's a good model.  





# §5. Embedding Visualization



Visualize and comment on the *embedding* that our model learned. 

We create an embedding in a relatively large number of dimensions (say, 10) and then use PCA to reduce the dimension down to a visualizable number. This procedure was demonstrated [in lecture](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-3.ipynb).



```python
# get the weights from the embedding layer
weights = model.get_layer('embedding').get_weights()[0]
# get the vocabulary from our data prep for later
vocab = vectorize_layer.get_vocabulary()

weights
```



The output below.

```python
array([[-0.02115575,  0.00996222],
       [-0.50931   ,  0.8828046 ],
       [ 5.560662  ,  1.0919735 ],
       ...,
       [ 2.6526396 ,  0.33504245],
       [ 1.6539431 ,  0.25953597],
       [ 2.2352915 ,  0.11224379]], dtype=float32)
```



We can see that the weights is 3-dimensional.  

Now,  we can use PCA to plot 2 dimensions. Here is the code.

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding_df
```



The output table below.



Then, Let's draw the plot.

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 6,
                 hover_name = "word")
# show the figure
fig.show()
```



**Hope you enjoyed this CNN learning process!**
